+++
title = "P2P Loan Default Prediction System"
date = 2026-01-21T18:44:02+08:00
date_range = "Dec 2024 â€“ Dec 2025"
draft = false  
toc = false 
tags = ["Machine Learning", "PySpark", "Big Data", "FinTech", "Risk Management"]
categories = ["Projects"]
image = "/LC.png"
weight = 98    
+++

### **About**
With the increasing reliance on credit for daily essentials and major purchases, managing risk has become imperative for financial institutions. This project focuses on **Lending Club**'s dataset available online, one of the world's largest Peer-to-Peer (P2P) lending platforms, to address the critical challenge of **credit default prediction**. Using a dataset spanning 2007 to 2020, our team aimed to identify high-risk borrowers and forecast the likelihood of loan default, enabling the firm to minimize financial losses while maximizing profitable lending opportunities.

The core of this project involved building an end-to-end Big Data machine learning pipeline. We processed a massive dataset (1.7 GB) containing millions of loan records, tackling real-world challenges such as high dimensionality and severe class imbalance (where defaults represented only ~13% of data). By leveraging advanced resampling techniques and distributed computing, we developed a robust model capable of flagging potential defaulters with high recall.

Our analysis revealed that factors such as **hardship reasons**, **loan grades**, and **home ownership status** (specifically renters) were the strongest predictors of default. The final model offers a data-driven approach for Lending Club to refine its risk-pricing strategies, such as tightening criteria for lower-grade loans or offering competitive rates to stable borrower profiles.

### **Methodology**
* **Big Data Processing:** Utilized **PySpark** to handle data cleaning and preprocessing for over 2.9 million rows, including handling missing values and standardizing inconsistent formats.
* **Feature Engineering & Selection:** Created custom risk indicators like `credit_util_ratio` and `installment_to_income_ratio`. Applied **Principal Component Analysis (PCA)** to reduce 83 features down to 62, capturing 95% of the variance while reducing computational load.
* **Handling Imbalance:** Implemented hybrid resampling strategies, specifically **SMOTE + ENN** (Synthetic Minority Over-sampling Technique combined with Edited Nearest Neighbours), to improve the model's ability to detect the minority "default" class.
* **Predictive Modeling:** Trained and evaluated six different classifiers including Logistic Regression, Random Forest, and Gradient-Boosted Trees (GBT).
* **Optimization:** The **Random Forest** model was selected as the best performer, achieving a **Recall of 0.82**, ensuring that the vast majority of potential defaulters are correctly identified.

### **Technologies Stack**
* **Python**
* **PySpark** (Big Data Processing & ML)
* **Google Colab Pro** (Cloud Computing Environment)
* **Imbalanced-learn** (SMOTE, ADASYN, Tomek Links)
* **Pandas & NumPy** (Data Manipulation)
* **Matplotlib & Seaborn** (Data Visualization)
